{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw-w3zSLWfZr",
        "outputId": "e005f7aa-88ad-4c65-a072-c090a3ea56b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  pickle.zip\n",
            "   creating: pickle/\n",
            "  inflating: pickle/.DS_Store        \n",
            "  inflating: __MACOSX/pickle/._.DS_Store  \n",
            "   creating: pickle/DHFR/\n",
            "   creating: pickle/CRT/\n",
            "   creating: pickle/DHPS/\n",
            "  inflating: pickle/DHFR/trimmed_data_GB4.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_DD2.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_KH2.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_HB3.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_KH1.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_mix1.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_mix2.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_3D7.pkl  \n",
            "  inflating: pickle/DHFR/trimmed_data_7G8.pkl  \n",
            "  inflating: pickle/CRT/trimmed_data_DD2.pkl  \n",
            "  inflating: pickle/CRT/trimmed_data_KH2.pkl  \n",
            "  inflating: pickle/CRT/trimmed_data_KH1.pkl  \n",
            "  inflating: pickle/CRT/trimmed_data_mix1.pkl  \n",
            "  inflating: pickle/CRT/trimmed_data_mix2.pkl  \n",
            "  inflating: pickle/CRT/trimmed_data_3D7.pkl  \n",
            "  inflating: pickle/CRT/trimmed_data_7G8.pkl  \n",
            "  inflating: pickle/DHPS/trimmed_data_DD2.pkl  \n",
            "  inflating: pickle/DHPS/trimmed_data_KH2.pkl  \n",
            "  inflating: pickle/DHPS/trimmed_data_HB3.pkl  \n",
            "  inflating: pickle/DHPS/trimmed_data_KH1.pkl  \n",
            "  inflating: pickle/DHPS/trimmed_data_mix1.pkl  \n",
            "  inflating: pickle/DHPS/trimmed_data_mix2.pkl  \n",
            "  inflating: pickle/DHPS/trimmed_data_3D7.pkl  \n"
          ]
        }
      ],
      "source": [
        "# Unpack the pickle file (Mainly a Colab thing)\n",
        "!unzip pickle.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import HaploReader as hp\n",
        "import VariantCalling as vc\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hCtadU33W27u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User-defined parameteres"
      ],
      "metadata": {
        "id": "lVhvhzEgc8LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_sequence = \"CRT\"\n",
        "nb_epoch = 100\n",
        "batch_size = 128\n",
        "nb_filters = 48\n",
        "nb_conv = 5\n",
        "nb_layer_conv = 3\n",
        "nb_pool = 3\n",
        "nb_alignment = 2500\n",
        "nb_channel = 5\n",
        "nb_coverage = 100\n",
        "nb_node = 128\n",
        "nb_layer_ff = 2\n",
        "train_loop = 0"
      ],
      "metadata": {
        "id": "kYiJco-kXIsR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the library for the corresponding parameters for each sequence"
      ],
      "metadata": {
        "id": "ae0QgThMcv4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_sequence == \"CRT\":\n",
        "    img_row, img_col = nb_coverage + 1, 178\n",
        "    clone_names = [\"3D7\",\"7G8\",\"DD2\"]\n",
        "    nb_mutations = len(clone_names)\n",
        "    ref_file_name = \"clones.txt\"\n",
        "elif run_sequence == \"DHPS\":\n",
        "    img_row, img_col = nb_coverage + 1, 642\n",
        "    clone_names = [\"3D7\",\"DD2\",\"HB3\"]\n",
        "    nb_mutations = len(clone_names)\n",
        "    ref_file_name = \"dhps_clones.txt\"\n",
        "elif run_sequence == \"DHFR\":\n",
        "    img_row, img_col = nb_coverage + 1, 491\n",
        "    clone_names = [\"3D7\",\"7G8\",\"DD2\",\"HB3\"]\n",
        "    nb_mutations = len(clone_names)\n",
        "    ref_file_name = \"dhfr_clones.txt\"\n",
        "#  Concatenate this section in the future for other genome sequences"
      ],
      "metadata": {
        "id": "vjZovCojWxBE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthesize the pile-up data"
      ],
      "metadata": {
        "id": "Lr2Sne_5c-ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dg = vc.VariantCallingData(file_name=ref_file_name,gen_mode=2,pkl_sequence=run_sequence,pkl_clones=clone_names)\n",
        "alignments_raw, prob_lists = dg.simulate_clones(nb_alignment,nb_coverage,0.01,0.01,verbose=1)\n",
        "prob_lists = (np.array(prob_lists)*100).tolist()\n",
        "alignments = [dg._array_constituent_bp(i,101,True) for i in alignments_raw]\n",
        "alignments = dg.char_to_int(alignments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf1LYruaW1-5",
        "outputId": "0979299f-4151-4123-8edc-749dbf22fbc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done, Number of alignments: 2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a model-builder object based on the user-defined parameters"
      ],
      "metadata": {
        "id": "r3GvLPQDdCCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ref_train = hp.HaploReaderTFTrain(alignments, prob_lists, nb_epoch, batch_size, nb_filters, nb_conv, nb_layer_conv, nb_pool, nb_node, nb_layer_ff, train_loop)"
      ],
      "metadata": {
        "id": "1yhWBSbuW2aD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(alignments)[0])\n",
        "print(np.shape(ref_train.prob_lists)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-CnxT6p60Jl",
        "outputId": "e96e3620-adc7-4414-cb46-f6229a8466b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform train-val split followed by model train\n",
        "The model training shall be run without issue"
      ],
      "metadata": {
        "id": "_ttoH5u7dGuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ref_train.train_val_split()\n",
        "ref_train.model_train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THBOprf7ZwBU",
        "outputId": "74334c0b-f728-4f49-e788-ebe7b76965a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 14s 101ms/step - loss: 109.8843 - mse: 312.7353 - val_loss: 109.8214 - val_mse: 314.7608\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 109.8733 - mse: 312.5061 - val_loss: 109.8496 - val_mse: 315.3971\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 109.8123 - mse: 311.1409 - val_loss: 109.7860 - val_mse: 313.9890\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 109.7011 - mse: 308.6612 - val_loss: 109.3676 - val_mse: 304.6934\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 108.2359 - mse: 275.8175 - val_loss: 104.8807 - val_mse: 203.2970\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 103.5111 - mse: 172.7669 - val_loss: 100.7028 - val_mse: 115.6345\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 100.1940 - mse: 105.7276 - val_loss: 97.8441 - val_mse: 62.5621\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 98.8540 - mse: 79.5687 - val_loss: 97.2448 - val_mse: 53.4235\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 84ms/step - loss: 98.1123 - mse: 66.1303 - val_loss: 96.8839 - val_mse: 47.7882\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 79ms/step - loss: 97.6890 - mse: 59.1301 - val_loss: 96.6792 - val_mse: 44.6522\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 97.4726 - mse: 55.3224 - val_loss: 96.5948 - val_mse: 42.3521\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 97.2794 - mse: 52.7425 - val_loss: 96.4308 - val_mse: 40.8694\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 96.8964 - mse: 46.3159 - val_loss: 96.3082 - val_mse: 38.8355\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 96.9218 - mse: 45.6979 - val_loss: 96.2181 - val_mse: 36.7856\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 96.7007 - mse: 42.6458 - val_loss: 96.0565 - val_mse: 34.5811\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 96.5606 - mse: 39.4080 - val_loss: 95.9452 - val_mse: 32.8429\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 96.4398 - mse: 37.1031 - val_loss: 96.1172 - val_mse: 35.6463\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 96.4528 - mse: 37.4662 - val_loss: 96.1300 - val_mse: 35.8613\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 96.2611 - mse: 33.5700 - val_loss: 95.7822 - val_mse: 29.7964\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 96.1555 - mse: 32.4755 - val_loss: 95.7858 - val_mse: 29.8567\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 96.0320 - mse: 30.6926 - val_loss: 95.7703 - val_mse: 30.2436\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 96.0321 - mse: 30.1885 - val_loss: 95.7812 - val_mse: 30.7667\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 96.0384 - mse: 30.1803 - val_loss: 95.9025 - val_mse: 32.9831\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.9887 - mse: 29.6527 - val_loss: 95.7641 - val_mse: 29.6332\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 95.9419 - mse: 28.3604 - val_loss: 95.6799 - val_mse: 28.0814\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 95.8708 - mse: 27.4447 - val_loss: 95.9176 - val_mse: 32.2311\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 95.8475 - mse: 26.8812 - val_loss: 95.7337 - val_mse: 29.2640\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.7803 - mse: 25.9648 - val_loss: 95.6247 - val_mse: 27.4255\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.6873 - mse: 23.8786 - val_loss: 95.6220 - val_mse: 27.3632\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.5844 - mse: 22.8806 - val_loss: 95.6461 - val_mse: 28.2424\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.5945 - mse: 23.1494 - val_loss: 95.6954 - val_mse: 29.4762\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.6062 - mse: 23.4027 - val_loss: 95.6084 - val_mse: 26.8679\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.6568 - mse: 23.8318 - val_loss: 95.7382 - val_mse: 29.9783\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.5687 - mse: 22.7504 - val_loss: 95.6174 - val_mse: 27.7188\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.5715 - mse: 22.7562 - val_loss: 95.6315 - val_mse: 27.9619\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.6098 - mse: 22.8119 - val_loss: 95.5844 - val_mse: 27.1619\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.5160 - mse: 21.8868 - val_loss: 95.8350 - val_mse: 31.5647\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.6151 - mse: 23.3637 - val_loss: 95.6829 - val_mse: 28.7223\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 95.5390 - mse: 22.5508 - val_loss: 95.6715 - val_mse: 28.3725\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.5558 - mse: 22.4260 - val_loss: 95.5948 - val_mse: 27.1162\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 95.5611 - mse: 22.3958 - val_loss: 95.8191 - val_mse: 30.9111\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 95.5162 - mse: 21.7508 - val_loss: 95.6162 - val_mse: 28.0340\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 95.4724 - mse: 21.6044 - val_loss: 95.6814 - val_mse: 28.2654\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.4271 - mse: 19.9796 - val_loss: 95.5969 - val_mse: 27.1299\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.4520 - mse: 20.9897 - val_loss: 95.7027 - val_mse: 29.5951\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.4200 - mse: 20.3031 - val_loss: 95.5935 - val_mse: 26.8347\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.3987 - mse: 19.3828 - val_loss: 95.6838 - val_mse: 28.1727\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.3379 - mse: 18.5272 - val_loss: 95.8377 - val_mse: 32.0727\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.3839 - mse: 19.6088 - val_loss: 95.6113 - val_mse: 27.5884\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.3593 - mse: 18.7533 - val_loss: 95.5614 - val_mse: 26.4826\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.3013 - mse: 18.1246 - val_loss: 95.7111 - val_mse: 29.4830\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.3342 - mse: 18.5857 - val_loss: 95.5566 - val_mse: 26.7368\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.3325 - mse: 18.7319 - val_loss: 95.6231 - val_mse: 27.1632\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.3173 - mse: 18.9373 - val_loss: 95.6928 - val_mse: 28.5028\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.3349 - mse: 18.6902 - val_loss: 95.6471 - val_mse: 27.8555\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.3097 - mse: 17.9750 - val_loss: 95.6594 - val_mse: 28.1439\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.2596 - mse: 17.4281 - val_loss: 95.7238 - val_mse: 29.6251\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 1s 71ms/step - loss: 95.2795 - mse: 17.5307 - val_loss: 95.6586 - val_mse: 27.8987\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 1s 71ms/step - loss: 95.2557 - mse: 17.2574 - val_loss: 95.6163 - val_mse: 27.6246\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 1s 50ms/step - loss: 95.2487 - mse: 17.0728 - val_loss: 95.6455 - val_mse: 28.4251\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.2733 - mse: 17.4427 - val_loss: 95.5891 - val_mse: 27.0611\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.2325 - mse: 16.2725 - val_loss: 95.6847 - val_mse: 28.5240\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.2374 - mse: 17.0360 - val_loss: 95.7093 - val_mse: 28.8895\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.2573 - mse: 17.5159 - val_loss: 95.5543 - val_mse: 26.6833\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 95.2005 - mse: 16.2882 - val_loss: 95.6562 - val_mse: 27.7836\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.2168 - mse: 16.4165 - val_loss: 95.5953 - val_mse: 27.6946\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.1975 - mse: 16.2507 - val_loss: 95.5930 - val_mse: 26.8225\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 95.2125 - mse: 16.7632 - val_loss: 95.7727 - val_mse: 29.6687\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.1600 - mse: 15.5407 - val_loss: 95.6015 - val_mse: 27.2438\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.2506 - mse: 17.3106 - val_loss: 95.6498 - val_mse: 28.0090\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.2447 - mse: 16.9657 - val_loss: 95.7361 - val_mse: 29.6053\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 95.1954 - mse: 16.2064 - val_loss: 95.6146 - val_mse: 27.5043\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.1774 - mse: 16.1666 - val_loss: 95.7624 - val_mse: 29.2733\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 1s 69ms/step - loss: 95.2133 - mse: 16.4732 - val_loss: 95.5594 - val_mse: 26.0293\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 1s 74ms/step - loss: 95.1898 - mse: 16.1415 - val_loss: 95.6464 - val_mse: 27.8330\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 1s 49ms/step - loss: 95.1677 - mse: 15.9056 - val_loss: 95.6945 - val_mse: 27.7393\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.0909 - mse: 14.1919 - val_loss: 95.7556 - val_mse: 29.4251\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.1497 - mse: 15.6206 - val_loss: 95.6896 - val_mse: 28.2986\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 95.1523 - mse: 15.7949 - val_loss: 95.7021 - val_mse: 28.0557\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 95.1437 - mse: 15.6821 - val_loss: 95.6634 - val_mse: 27.8190\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.1402 - mse: 15.5981 - val_loss: 95.6394 - val_mse: 27.4919\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.1275 - mse: 15.0996 - val_loss: 95.8036 - val_mse: 30.0817\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 95.1191 - mse: 14.8761 - val_loss: 95.7224 - val_mse: 28.9611\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.0382 - mse: 13.6604 - val_loss: 95.6986 - val_mse: 28.4258\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 95.0845 - mse: 14.4301 - val_loss: 95.7313 - val_mse: 29.2597\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.0541 - mse: 13.7246 - val_loss: 95.7015 - val_mse: 28.5847\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.1269 - mse: 14.8614 - val_loss: 95.7286 - val_mse: 29.2276\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.0981 - mse: 14.3732 - val_loss: 95.7416 - val_mse: 29.0835\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.1182 - mse: 14.9360 - val_loss: 95.6443 - val_mse: 27.7899\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 95.0799 - mse: 14.3904 - val_loss: 95.7283 - val_mse: 28.5493\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 1s 71ms/step - loss: 95.0173 - mse: 13.4244 - val_loss: 95.7719 - val_mse: 29.7798\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.0455 - mse: 13.7240 - val_loss: 95.6687 - val_mse: 28.0757\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.0430 - mse: 13.8055 - val_loss: 95.7544 - val_mse: 29.8076\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 95.1077 - mse: 14.5189 - val_loss: 95.6966 - val_mse: 28.3153\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.0894 - mse: 14.0581 - val_loss: 95.7217 - val_mse: 28.8594\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.0477 - mse: 13.6566 - val_loss: 95.7968 - val_mse: 29.9274\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 95.0337 - mse: 13.6427 - val_loss: 95.7450 - val_mse: 29.3084\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 95.0253 - mse: 13.2661 - val_loss: 95.7995 - val_mse: 30.2089\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 95.0581 - mse: 14.2840 - val_loss: 95.8006 - val_mse: 30.4116\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 95.0084 - mse: 13.2256 - val_loss: 95.7786 - val_mse: 29.8871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ref_train.model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLztOqjUdUqV",
        "outputId": "9c0d8929-2752-4004-eb5a-09cc7026ab96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 21, 36, 48)        6048      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 7, 12, 48)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 7, 12, 48)         0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 7, 12, 48)         0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 2, 3, 48)          57648     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 1, 1, 48)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1, 1, 48)          0         \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1, 1, 48)          0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 1, 48)          57648     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 48)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1, 1, 48)          0         \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1, 1, 48)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 48)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               6272      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 3)                 0         \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 144,515\n",
            "Trainable params: 144,515\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "The advantage of this setup is the object can be modified to take another alingment pile-up data without the need to repeat the code for the model definition"
      ],
      "metadata": {
        "id": "dEXDOyx1dZla"
      }
    }
  ]
}