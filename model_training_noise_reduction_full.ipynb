{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VariantCalling as vc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Input, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = vc.VariantCallingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if GPU is available\n",
    "# if tf.config.experimental.list_physical_devices('GPU'):\n",
    "#     # Limit GPU memory allocation to avoid GPU memory exhaustion\n",
    "#     for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# else:\n",
    "#     print(\"No GPU found. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model1(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoded = Conv2D(64, (8, 8), activation='relu', padding='same')(input_layer)\n",
    "    encoded = Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    decoded = Conv2DTranspose(64, (8, 8), activation='relu', padding='same')(decoded)\n",
    "    decoded = Conv2D(1, (8, 8), activation='sigmoid', padding='same')(decoded)\n",
    "\n",
    "    model = Model(input_layer, decoded)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_cnn_model2(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoded = Conv2D(64, (5, 5), activation='relu', padding='same')(input_layer)\n",
    "    encoded = Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    decoded = Conv2DTranspose(64, (5, 5), activation='relu', padding='same')(decoded)\n",
    "    decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same')(decoded)\n",
    "\n",
    "    model = Model(input_layer, decoded)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(align_err, seq_err):\n",
    "    img_row = 100\n",
    "    img_col = 178\n",
    "    \n",
    "    print(f\"\\ngenerating alignment error {align_err}, seq error {seq_err}\")\n",
    "    noisy, clean = dg.generate_data_for_noise_reduction(sample_size=15000, alignment_error_prob=align_err, sequencing_error_prob=seq_err)\n",
    "    \n",
    "    noisy = noisy.astype('float32') / 3\n",
    "    clean = clean.astype('float32') / 3\n",
    "    noisy = noisy.reshape(noisy.shape[0], img_row, img_col, 1)\n",
    "    clean = clean.reshape(clean.shape[0], img_row, img_col, 1)\n",
    "    return noisy, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 02:56:19.349237: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-09-02 02:56:19.349262: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-09-02 02:56:19.349267: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-09-02 02:56:19.349297: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-09-02 02:56:19.349314: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model1:\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 178, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 100, 178, 64)      4160      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 100, 178, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 100, 178, 128)     147584    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 100, 178, 64)      524352    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 100, 178, 1)       4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 754049 (2.88 MB)\n",
      "Trainable params: 754049 (2.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "model2:\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 100, 178, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 100, 178, 64)      1664      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 100, 178, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 100, 178, 128)     147584    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 100, 178, 64)      204864    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 100, 178, 1)       1601      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429569 (1.64 MB)\n",
      "Trainable params: 429569 (1.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (100, 178, 1)\n",
    "\n",
    "model1 = create_cnn_model1(input_shape)\n",
    "model2 = create_cnn_model2(input_shape)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "model1.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
    "print(\"\\nmodel1:\")\n",
    "print(model1.summary())\n",
    "\n",
    "learning_rate = 0.0005\n",
    "model2.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
    "print(\"\\nmodel2:\")\n",
    "print(model2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current module:  {'align_err': 0, 'seq_err': 0.01, 'model1_epochs': 8, 'model2_epochs': 4}\n",
      "\n",
      "generating alignment error 0, seq error 0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b65de814764770807cb60b1f4eead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 03:00:20.042275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 382s 814ms/step - loss: 0.0047\n",
      "Epoch 2/8\n",
      "175/469 [==========>...................] - ETA: 3:57 - loss: 5.3220e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m noisy, clean \u001b[39m=\u001b[39m gen_data(module[\u001b[39m\"\u001b[39m\u001b[39malign_err\u001b[39m\u001b[39m\"\u001b[39m], module[\u001b[39m\"\u001b[39m\u001b[39mseq_err\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     19\u001b[0m epochs \u001b[39m=\u001b[39m module[\u001b[39m\"\u001b[39m\u001b[39mmodel1_epochs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m model1\u001b[39m.\u001b[39mfit(noisy, clean, batch_size\u001b[39m=\u001b[39mbatch_size, epochs\u001b[39m=\u001b[39mepochs, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m denoised_image1 \u001b[39m=\u001b[39m model1\u001b[39m.\u001b[39mpredict(noisy)\n\u001b[1;32m     23\u001b[0m denoised_image1_quantised \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(denoised_image1 \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1748\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1746\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1747\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1748\u001b[0m callbacks\u001b[39m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1749\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1750\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39mTRAIN, \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m, batch, logs\u001b[39m=\u001b[39mlogs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/tf_utils.py:694\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 694\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py:624\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnest.map_structure\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_structure\u001b[39m(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    540\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[39m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m   \u001b[39mreturn\u001b[39;00m nest_util\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    625\u001b[0m       nest_util\u001b[39m.\u001b[39mModality\u001b[39m.\u001b[39mCORE, func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    626\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1054\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[39m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mCORE:\n\u001b[0;32m-> 1054\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1055\u001b[0m \u001b[39melif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mDATA:\n\u001b[1;32m   1056\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/tf_utils.py:687\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 687\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numpy_internal()\n\u001b[1;32m   1108\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Curriculum\n",
    "batch_size = 32\n",
    "img_row = 100\n",
    "img_col = 178\n",
    "\n",
    "curriculum = [\n",
    "    {\"align_err\": 0, \"seq_err\": 0.01, \"model1_epochs\": 8, \"model2_epochs\": 4},\n",
    "    {\"align_err\": 0, \"seq_err\": 0.05, \"model1_epochs\": 5, \"model2_epochs\": 3},\n",
    "    {\"align_err\": 0.001, \"seq_err\": 0.05, \"model1_epochs\": 7, \"model2_epochs\": 3},\n",
    "    {\"align_err\": 0.003, \"seq_err\": 0.05, \"model1_epochs\": 5, \"model2_epochs\": 3},\n",
    "    {\"align_err\": 0.005, \"seq_err\": 0.05, \"model1_epochs\": 5, \"model2_epochs\": 2},\n",
    "    {\"align_err\": 0.01, \"seq_err\": 0.05, \"model1_epochs\": 5, \"model2_epochs\": 2},\n",
    "]\n",
    "\n",
    "for module in curriculum:\n",
    "    print(\"current module: \", module)\n",
    "    noisy, clean = gen_data(module[\"align_err\"], module[\"seq_err\"])\n",
    "\n",
    "    epochs = module[\"model1_epochs\"]\n",
    "    model1.fit(noisy, clean, batch_size=batch_size, epochs=epochs, shuffle=True)\n",
    "\n",
    "    denoised_image1 = model1.predict(noisy)\n",
    "    denoised_image1_quantised = np.round(denoised_image1 * 3) / 3\n",
    "\n",
    "    epochs = module[\"model2_epochs\"]\n",
    "    model2.fit(denoised_image1_quantised, clean, batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
